% !TeX root = thesis.tex
% !TeX spellcheck = en_US
% !TeX encoding = UTF-8
\documentclass[english, LaM, oneside]{sapthesis}%remove "english" for a thesis written in Italian

%\usepackage[utf8]{inputenx}
%\usepackage{xcolor}
%\usepackage{indentfirst}
%\usepackage{microtype}

%\usepackage{lettrine}
%\linespread{0.9}

% This can be used to make space between section names more compact. The titlesec package allows changing how chapters are displayed, numerated, etc.
%\usepackage[compact]{titlesec}
% to get the bibliography in the toc
\usepackage[nottoc,notlot,notlof,chapter]{tocbibind}

\newcommand{\thesistitle}{{\sc ltl} and Past {\sc ltl} on Finite Traces for Planning and Declarative Process Mining}

\usepackage{index}
\usepackage{hyperref}
\usepackage{wrapfig}
\usepackage[round]{natbib}
\bibliographystyle{plainnat}

\usepackage[intoc,refpage]{nomencl} %refeq
\makenomenclature

%\usepackage{qtree}
% The algorithm packages have to be after hyperref.
\usepackage{algorithm}
\usepackage{algpseudocode}

\algnewcommand\algorithmicforeach{\textbf{foreach}}
\algdef{S}[FOR]{ForEach}[1]{\algorithmicforeach\ #1\ \algorithmicdo}

\usepackage{mathtools}
\usepackage{amssymb, amsmath, amsthm}
\usepackage{xspace, float, graphicx, pstricks}
%\usepackage{mathabx}

\usepackage{caption}% http://ctan.org/pkg/caption
\captionsetup[ruled]{labelsep=period}
\makeatletter
\@addtoreset{algorithm}{chapter}% algorithm counter resets every chapter
\makeatother
\renewcommand{\thealgorithm}{\thechapter.\arabic{algorithm}}% Algorithm # is <chapter>.<algorithm>

\usepackage{subcaption}
%\usepackage[autostyle]{csquotes}  
\usepackage{graphicx}
%\graphicspath{{./images/}}
\usepackage{rotating}

\usepackage{tikz}
\usetikzlibrary{automata,positioning,arrows, shapes}
%
%\tikzstyle{mine}[node distance=2.5cm, % Minimum distance between two nodes. 
%		 shorten >=1pt,
%         every state/.style={ % Sets the properties for each state
%         semithick,
%         fill=gray!10},
%         initial text={},     % No label on start arrow
%         double distance=2pt, % Adjust appearance of accept states
%         every edge/.style={  % Sets the properties for each transition
%         draw,
%         ->,>=stealthâ€™,     % Makes edges directed with bold arrowheads
%         auto,
%         semithick}
%]      

\usepackage{listings}
% Custom colors
\usepackage{color}
\definecolor{deepblue}{rgb}{0,0,0.5}
\definecolor{deepred}{rgb}{0.6,0,0}
\definecolor{deepgreen}{rgb}{0,0.5,0}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}

\usepackage{accsupp}    
\newcommand{\noncopynumber}[1]{
	\BeginAccSupp{method=escape,ActualText={}}
	#1
	\EndAccSupp{}
}
\lstdefinestyle{Python}{
	language        = Python,
	backgroundcolor=\color{backcolour},
	basicstyle      = \ttfamily,
	keywordstyle    = \color{deepblue},
	stringstyle     = \color{deepgreen},
	commentstyle    = \color{codegray}\ttfamily,
	numberstyle=\tiny\color{codegray}\noncopynumber,
	columns=flexible,
	numbers=left,
	stepnumber=1
}

\lstdefinestyle{Mona}{
	basicstyle      = \ttfamily,
	numberstyle=\tiny\color{codegray}\noncopynumber,	
	columns=flexible,
	numbers=left,
	stepnumber=1	
}

\lstdefinelanguage{PDDL}{
  basicstyle      = \ttfamily,
  sensitive=false,    % not case-sensitive
  morecomment=[l]{;}, % line comment
  alsoletter={:,-},   % consider extra characters
  numberstyle=\tiny\color{codegray}\noncopynumber,	
  numbers=left,
  stepnumber=1,
  morekeywords={
    define,domain,problem,not,and,or,when,forall,exists,either,
    :domain,:requirements,:types,:objects,:constants,
    :predicates,:action,:parameters,:precondition,:effect,
    :fluents,:primary-effect,:side-effect,:init,:goal,
    :strips,:adl,:equality,:typing,:conditional-effects,
    :negative-preconditions,:disjunctive-preconditions,
    :existential-preconditions,:universal-preconditions,:quantified-preconditions,
    :functions,assign,increase,decrease,scale-up,scale-down,
    :metric,minimize,maximize,
    :durative-actions,:duration-inequalities,:continuous-effects,
    :durative-action,:duration,:condition
  }
}

\hypersetup{
	hyperfootnotes=true,            
	bookmarks=true,         
	colorlinks=true,
	linkcolor=red,
	linktoc=page,
	anchorcolor=black,
	citecolor=red,
	urlcolor=blue,
	pdftitle={\thesistitle},
	pdfsubject = {Master Thesis, University of Rome "Sapienza"},
	pdfauthor={Francesco Fuggitti},
	pdfkeywords={master thesis, sapienza, roma, university, francesco fuggitti}
	pdfauthor = {\textcopyright\ \today\ Francesco Fuggitti},
}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[chapter] % reset theorem numbering for each chapter
\newtheorem{lemma}[theorem]{Lemma}
\theoremstyle{definition}
\newtheorem{definition}{Definition}[chapter]
\newtheorem{example}{Example}[chapter]

\newgeometry{twoside}

%opening
\title{\thesistitle}
\author{Francesco Fuggitti}
\IDnumber{1735212}
\course[override]{Master of Science in Engineering in Computer Science}
\courseorganizer{Faculty of Information Engineering, Informatics, and Statistics}
\submitdate{2017/2018}
\copyyear{2018}
\advisor{Prof. Giuseppe De Giacomo}
\authoremail{fuggitti.1735212@studenti.uniroma1.it}
\examdate{$31^{\text{st}}$ October 2018}
\examiner{Prof. }
\examiner{Prof. }
\examiner{Prof. }
\examiner{Prof. }
\examiner{Prof. }
\examiner{Prof. }
\examiner{Prof. }    

\allowdisplaybreaks

\begin{document}
	\input{macros}
	
	\frontmatter	
	\maketitle
	
%	\begin{abstract}
%		MDPs extended with \LTLf non-Markovian rewards
%		have recently attracted interest as a way to specify rewards	declaratively. In this thesis, we discuss how a reinforcement learning agent can learn policies fulfilling \LLf goals.
%		In particular we focus on the case where we have two separate representations of the world: one for the agent, using the
%		(predefined, possibly low-level) features available to it, and
%		one for the goal, expressed in terms of high-level (human-understandable) fluents. We formally define the problem and
%		show how it can be solved. Moreover, we provide experimental evidence that keeping the RL agent feature space separated from the goal's can work in practice, showing interesting cases where the agent can indeed learn a policy that fulfills the \LLf goal using only its features (augmented
%		with additional memory).
%	\end{abstract}
	
%\dedication{to my grandparents}
	
%	\begin{acknowledgments}
%	\end{acknowledgments}
	
	\tableofcontents
	
	\mainmatter
	\input{chapters/introduction}
	\input{chapters/ltl}
	\input{chapters/ltlf2dfa}
	\input{chapters/planning}
	\input{chapters/janus}
	\input{chapters/conclusions}
	
%	\appendix 
%	\input{chapters/appendix-flloat}
%	\input{chapters/appendix-rltg}

	
	\backmatter
	\phantomsection
	
	\bibliography{bib.bib}
	

\end{document}